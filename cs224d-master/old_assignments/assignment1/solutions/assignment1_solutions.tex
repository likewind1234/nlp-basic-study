% Template LaTeX source file for homework problem solutions.
% Alan T. Sherman (9/9/98)
% Updated: Greg King (2014)

% Running LaTeX
%
% Name this file FOO.tex
% latex FOO
% latex FOO   
%    (You have to run latex twice to get the cross references correct.
%     Running latex creates a file FOO.dvi 
%     You can view dvi files with the program xdvi )
% xdvi FOO.dvi &
%
% lpr -d FOO.dvi
%    (To print the dvi file.   Be sure to use the "-d" print option,
%     and be sure your printer can handle dvi files (not all printers can).
%     Do NOT print with "lpr FOO.dvi", which will print tens of pages
%     of unreadable dvi source code. Printing a postscript (ps) file
%     is usually more reliable, as explained below.)
%
% dvips FOO.dvi
%    (To create a postscript file named FOO.ps 
%     which you can view with the program ghostview )
% ghostview FOO.ps &
% lpr FOO.ps
%    (To print the ps file.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letter,12pt]{article}
\RequirePackage{amsmath}
\RequirePackage{amsmath,amssymb,amsthm}
\RequirePackage{tikz}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}

\renewcommand{\lstlistlistingname}{Code Listings}
\renewcommand{\lstlistingname}{Code Listing}
\definecolor{gray}{gray}{0.5}
\definecolor{key}{rgb}{0,0.5,0}
\lstnewenvironment{python}[1][]{
\lstset{
language=python,
basicstyle=\ttfamily\small,
otherkeywords={1, 2, 3, 4, 5, 6, 7, 8 ,9 , 0, -, =, +, [, ], (, ), \{, \}, :, *, !},
keywordstyle=\color{blue},
stringstyle=\color{red},
showstringspaces=false,
emph={class, pass, in, for, while, if, is, elif, else, not, and, or,
def, print, exec, break, continue, return},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{key},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
upquote=true,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray}\slshape,
frame=tb,
rulesepcolor=\color{blue},#1
}}{}


\usetikzlibrary{calc}
\RequirePackage{tkz-euclide}
\usetkzobj{all}
%\usepackage{minted}
%\usepackage{fontspec}
%\setsansfont{Calibri}
%\setmonofont{Consolas}
%    \begin{minted}[mathescape,
%                   linenos,
%                   numbersep=5pt,
%                   gobble=2,
%                   frame=lines,
%                   framesep=2mm]{csharp}
%      string title = "This is a Unicode Ï€ in the sky"
%      /*
%      Defined as $\pi=\lim_{n\to\infty}\frac{P_n}{d}$ where $P$ is the perimeter
%      of an $n$-sided regular polygon circumscribing a
%      circle of diameter $d$.
%      */
%      const double pi = 3.1415926535
%    \end{minted}


%\usepackage[utf8]{inputenc}
%
%% Default fixed font does not support bold face
%\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
%\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
%
%% Custom colors
%\usepackage{color}
%\definecolor{deepblue}{rgb}{0,0,0.5}
%\definecolor{deepred}{rgb}{0.6,0,0}
%\definecolor{deepgreen}{rgb}{0,0.5,0}
%
%\usepackage{listings}
%
%% Python style for highlighting
%\newcommand\pythonstyle{\lstset{
%language=Python,
%basicstyle=\ttm,
%otherkeywords={self},             % Add keywords here
%keywordstyle=\ttb\color{deepblue},
%emph={MyClass,__init__},          % Custom highlighting
%emphstyle=\ttb\color{deepred},    % Custom highlighting style
%stringstyle=\color{deepgreen},
%frame=tb,                         % Any extra options here
%showstringspaces=false            % 
%}}
%
%
%% Python environment
%\lstnewenvironment{python}[1][]
%{
%\pythonstyle
%\lstset{#1}
%}
%{}
%
%% Python for external files
%\newcommand\pythonexternal[2][]{{
%\pythonstyle
%\lstinputlisting[#1]{#2}}}
%
%% Python for inline
%\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
%
%\begin{document}
%
%\section{``In-text'' listing highlighting}
%
%\begin{python}
%class MyClass(Yourclass):
%    def __init__(self, my, yours):
%        bla = '5 1 2 3 4'
%        print bla
%\end{python}
%
%\section{External listing highlighting}
%
%\pythonexternal{demo.py}
%
%\section{Inline highlighting}
%
%Definition \pythoninline{class MyClass} means \dots
%
%\end{document}
%    \begin{minted}{python}
%    def boring(args = None):
%    pass
%    \end{minted}

% Set the margins
%
\setlength{\textheight}{8.5in}
\setlength{\headheight}{.25in}
\setlength{\headsep}{.25in}
\setlength{\topmargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros

% Math Macros.  It would be better to use the AMS LaTeX package,
% including the Bbb fonts, but I'm showing how to get by with the most
% primitive version of LaTeX.  I follow the naming convention to begin
% user-defined macro and variable names with the prefix "my" to make it
% easier to distiguish user-defined macros from LaTeX commands.
%
\newcommand{\myN}{\hbox{N\hspace*{-.9em}I\hspace*{.4em}}}
\newcommand{\myZ}{\hbox{Z}^+}
\newcommand{\myR}{\hbox{R}}

\newcommand{\myfunction}[3]
{${#1} : {#2} \rightarrow {#3}$ }

\newcommand{\myzrfunction}[1]
{\myfunction{#1}{{\myZ}}{{\myR}}}


% Formating Macros
%

\newcommand{\myheader}[4]
{\vspace*{-0.5in}
\noindent
{#1} \hfill {#3}

\noindent
{#2} \hfill {#4}

\noindent
\rule[8pt]{\textwidth}{1pt}

\vspace{1ex} 
}  % end \myheader 

\newcommand{\myalgsheader}[0]
{\myheader{Stanford University, Department of Computer Science}
{Computer Science 224D}{Spring 2016}{Section 1}}

% Running head (goes at top of each page, beginning with page 2.
% Must precede by \pagestyle{myheadings}.
\newcommand{\myrunninghead}[2]
{\markright{{\it {#1}, {#2}}}}

\newcommand{\myrunningalgshead}[2]
{\myrunninghead{Computer Science 224D}{{#1}}}

\newcommand{\myrunninghwhead}[2]
{\myrunningalgshead{Solution to Assignment {#1}, Problem {#2}}}

\newcommand{\mytitle}[1]
{\begin{center}
{\large {\bf {#1}}}
\end{center}}

\newcommand{\myhwtitle}[3]
{\begin{center}
{\large {\bf Solution to Assignment {#1}, Problem {#2}}}\\
\medskip 
{\it {#3}} % Name goes here
\end{center}}

\newcommand{\mysection}[1]
{\noindent {\bf {#1}}}

%%%%%% Begin document with header and title %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\myalgsheader

\pagestyle{plain}
\setcounter{page}{1}
\myhwtitle{1}{1}{Gregory King}

\bigskip
%Assn#1:
%1.29, 1.42, 1.43
%%%%% Begin Solution %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent {\bf Softmax} Prove that softmax (denoted $\textrm{softmax}({\bf x})$)
is invariant to a constant offset in the input, that is, for any input vector
$\bf x$ and any constant $c$,
\begin{equation}
\textrm{softmax}({\bf x}) = \textrm{softmax}({\bf x} + c),
\end{equation}
where ${\bf x} + c$ means adding the constant $c$ to every dimension of ${\bf x}$.

{\bf Note}: In practice, we make use of this property and choose $c = -\textrm{max}_{i}({\bf x_i})$, when computing $\textrm{softmax}$ probabilities for numerical stability (i.e. subtracting the maximum element from all elements of ${\bf x}$).\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\noindent Starting from the definition of softmax,
\begin{equation}
\textrm{softmax}({\bf x})_{i} = \frac{e^{x_{i}}}{\sum_{j=1}{e^{x_{j}}}}
\end{equation}
it follows, the linear shift, ${\bf x} + c$, yields the following (for the $i$ element):
\begin{align}
\textrm{softmax}({\bf x} + c)_{i} & = \frac{e^{(x_{i} + c)}}{\sum_{j=1}{e^{(x_{j} + c)}}}\\
                                                 & = \frac{e^{c}}{e^{c}}\frac{e^{(x_{i})}}{\sum_{j=1}{e^{(x_{j})}}}\\
                                                 & = \textrm{softmax}({\bf x})_{i}
\end{align}
since this is true for an arbitary $i$, it follows immediately that the linear shift has no affect on the softmax probabilities.


%\sum_{j=1}{\exp{x_{j}}

% The vector addition diagram is drawn in Figure~\ref{solution figure:question 1.29}.
% Careful measurement gives $\vec{D}$ is around 144~m, $41^{\circ}$ south of west (scale is 1 grid point:50 m).
% \begin{figure}[!h!b]
% \begin{center}
% \begin{tikzpicture}
%   \draw[step=.5cm,gray,very thin](-2.,-2.) grid (2., 2.);
%   \draw[->,black,thick] (-2.,0) -- (2.,0) coordinate (x axis);
%   \draw[->,black,thick] (0,-2.) -- (0,2.) coordinate (y axis);
%   \coordinate (A) at (0,0);%start at orgin
%   \coordinate (B) at (-1.8,0);%go west 180 m
%   \coordinate (C) at (-0.315,-1.485);% go east 210 cos (315*) = 148.5m, go south 148.5m
%   \coordinate (D) at (1.085, 0.94);%go north 280 sin (60*) = 242.5 m, go east 280 sin(60*) = 140m
%   \draw[->,black,very thick] (A) -- node[above=2pt] {$\vec{A}$} (B);
%   \draw[->,black,very thick] (B) -- node[left=2pt] {$\vec{B}$}  (C);
%   \draw[->,black,very thick] (C) -- node[right=2pt] {$\vec{C}$} (D);
%   \draw[->,black,dashed, very thick] (D) -- node[above=2pt] {$\vec{D}$} (A);
% \end{tikzpicture}
% \end{center}\caption{Vector Diagram for (1.29)\label{solution figure:question 1.29}}
% \end{figure}

%% First page break
%%   Note: "myheadings" is a LaTeX keyword; it is not a user-defined entity.
%%
\clearpage
\pagestyle{myheadings}



\myrunninghwhead{1}{2 (Neural Networks)}

\myhwtitle{1}{2(a)}{Gregory King}

\bigskip

\noindent Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the 
function value (i.e. in some expression where only $\sigma(x)$, but not $x$, is present). Assume that the input
$x$ is a scalar for this question.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
 
\noindent Denote the sigmoid function as $\sigma(z)$,
\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}}.
\end{equation}
It follows, using the chain rule (and noting that $e^{-z}= 1/\sigma(z) -1$),
\begin{align*}
\sigma^{\prime}(z)   &= \frac{-1}{(1+e^{-z})^{2}}\times(-e^{-z})\\
                                &= \sigma^{2}(z)\left(\frac{1}{\sigma(z)} - 1 \right)\\
                                &= \sigma(z) - \sigma^{2}(z)\\
                                &= \sigma(z)(1 - \sigma(z))
\end{align*}
 
\clearpage

\myhwtitle{1}{2(b)}{Gregory King}
\bigskip

\noindent Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for
evaluation, i.e. find the gradients with respect to the softmax input vector $\theta$, when the prediction is
made by $\hat{y} = \textrm{softmax}(\theta)$. Remember the cross entropy function is
\begin{equation}
\textrm{CE}({\bf y},{\bf\hat{y}}) = -\sum_{i}{y_{i}\log{\hat{y_{i}}}}
\end{equation}
where $y$ is the one-hot label vector, and $\hat{y}$ is the predicted probability vector for all classes. \vspace{2mm}\\
\noindent {\bf Hint}: you might want to consider the fact many elements of $y$ are zeros, and assume that only the $k$-th dimension
of $y$ is one.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
Starting with, cross entropy,
\begin{equation}
\textrm{CE}({\bf y},{\bf\hat{y}}) = -\sum_{i}{y_{i}\log{\hat{y_{i}}}}
\end{equation}
We can derive the gradient of the cross-entropy function, using back propagation, 
\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\hat{y}_{i}}} = -\frac{y_{j}}{\hat{y}_{i}}\label{eq:2b 1}
\end{equation}
Thus, 
\begin{align}
\frac{\partial(\textrm{CE})}{\partial{\theta_{k}}} = & \frac{\partial(\textrm{CE})}{\partial{\hat{y}_{i}}}\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}} \\
                                                                        = & -\frac{y_{j}}{\hat{y}_{i}}\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}}
\end{align}
Calculating the partial derivative of $\hat{y_{i}}$ (where $i=k$)
\begin{align}
\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{i}}} = & \frac{\partial}{\partial{\theta_{i}}}\left( \frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}}\right) \\
                                                                      = & \frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}} - \left(\frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}}\right)^{2} \\
                                                                      = & \hat{y}_{i}\cdot(1 - \hat{y}_{i})\label{eq:2b 2}
\end{align}
and (where $i\neq k$),
\begin{align}
\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}} = & \frac{\partial}{\partial{\theta_{k}}}\left( \frac{e^{\theta_{i}}}{\sum_{j=1}{e^{\theta_{j}}}}\right) \\
                                                                       = & - \left(\frac{e^{\theta_{i}}e^{\theta_{k}}}{\sum_{j=1}{e^{\theta_{j}}}}\right) \\
                                                                       = & - \hat{y}_{i}\hat{y}_{k}\label{eq:2b 3}
\end{align}
Combining Equations \ref{eq:2b 1}, \ref{eq:2b 2}, \ref{eq:2b 3}, yields 
\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\theta_{k}}} = \begin{cases}
-y_{j}(1 - \hat{y}_{k})&\text{ for }i=k \\
y_{j}\hat{y}_{k}&\text{ for }i\neq k
\end{cases}
\end{equation}
Requiring $y_{j}$ to be non-zero, imposes that the auxiliary condition, $k=j$ and $y_{j}=1$, hence it follows immediately,
\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\theta_{j}}} = \begin{cases}
(\hat{y}_{j} - 1)&\text{ for }i=j\label{eq:2b case 1} \\
\hat{y}_{j}&\text{ for }i\neq j
\end{cases}
\end{equation}
Which is equivalent to (with the substitution $y_{j}$ for $1$ in the first case of Equation \ref{eq:2b case 1}),
\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\boldsymbol\theta}} = \bf{\hat{y}} - \bf{y}
\end{equation}

\clearpage

\myhwtitle{1}{2(c)}{Gregory King}
\bigskip

\noindent Derive the gradients with respect to the inputs x to an one-hidden-layer neural network (that is, find
$\partial{J}/\partial{\bf x}$ where $J$ is the cost function for the neural network). The neural network employs sigmoid activation
function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is $y$,
and cross entropy cost is used. (feel free to use $\sigma^{\prime}(x)$ as the shorthand for sigmoid gradient, and feel free
to define any variables whenever you see fit). \vspace{5mm}\\
\def\layersep{2.5cm}
\begin{center}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:$x_{\y}$] (I-\name) at (0,-\y) {};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,5}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

    % Draw the output layer nodes
    \foreach \name / \y in {1,...,6}
        \path[yshift=0.75cm]
            node[output neuron] (O-\name) at (\layersep+\layersep,-\y cm) {};
%    % Draw the output layer node
%    \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] (O) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,5}
            \path (I-\source) edge (H-\dest);

%    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,5}
       \foreach \dest in {1,...,6}
        \path (H-\source) edge (O-\dest);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl) {$h$};
    \node[annot,left of=hl] {$x$};
    \node[annot,right of=hl] {$\hat{y}$};
\end{tikzpicture}
\end{center}
\noindent Recall that the forward propagation is as follows:
\begin{align}
{\bf h}         & = \textrm{sigmoid}({\bf x\textrm{W}_{1} + b_{1}}) \\
{\bf \hat{y}} & = \textrm{softmax}({\bf h\textrm{W}_{2} + b_{2}})
\end{align}
\noindent {\bf Note}: here we're assuming that the input vector (and thus the hidden variables and output probabilities)
is a row vector to be consistent with the programming part of the assignment. When we apply the sigmoid function to
a vector, we are applying it to each of the elements of the vector. $\bf{\textrm{W}}_i$ and $\bf{b}_{i}$ ($i\in\{1,2\}$) are
the weights and biases, respectively of the two layers.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
In order to simplify the notation used to solve the problem, define the following terms:
\begin{align}
{\bf x}^{(2)} \equiv& \quad{\bf h} \\
{\bf z}_{i}     \equiv& \quad{\bf x^{(i)}\textrm{W}_{i} + b_{i}}
\end{align}
Now, to calculate $\partial{J}/\partial{\bf x^{1}}$, one can use the back propagation algorithm. Starting with the results from Question 2(b):
%\begin{equation}
%\frac{\partial J}{\partial{x^{1}_{i}}} = \frac{\partial J}{\partial(\textrm{softmax})}\frac{\partial(\textrm{softmax})}{\partial {\bf x^{2}}}\frac{\partial {\bf x^{2}}}{\partial\sigma}\frac{\partial\sigma}{\partial{x^{1}_{i}}}
%\end{equation}
\begin{equation}
\frac{\partial{J}}{\partial{\bf z}_{2}} = \bf{\hat{y}} - \bf{y}
\end{equation}
and
\begin{equation}
\frac{\partial{\bf z}_{i}}{\partial{{\bf x}^{(i)}}} = {\bf \textrm{W}^{\top}_{i}}\label{eq: 2c 1}
\end{equation}
Sigmoid ($\sigma$) derivative can be found in Question 2(a), but we define:
\begin{equation}
\frac{\partial{{\bf x}^{(2)}}}{\partial{{\bf z}_{1}}}\equiv\sigma^{\prime}(z_{1})%{\bf x}^{(2)}\cdot(1  - {\bf x}^{(2)})
\end{equation}
Combining these, and using $\cdot$ to denote element-wise product:
\begin{equation}
\frac{\partial{J}}{\partial{z_{i}}} = (\bf{\hat{y}} - \bf{y}){\bf \textrm{W}^{\top}_{2}}\cdot\sigma^{\prime}(z_{1})
\end{equation}
Finally, using the results from Equation~\ref{eq: 2c 1} (but for the first layer):
\begin{equation}
\frac{\partial{J}}{\partial{{\bf x}^{(1)}}} = (\bf{\hat{y}} - \bf{y}){\bf \textrm{W}^{\top}_{2}}\cdot\sigma^{\prime}(z_{1})\cdot{\bf \textrm{W}^{\top}_{1}}
\end{equation}
%Verifying the dimensionality, expect $\textrm{\bf dim}\left(\frac{\partial{J}}{\partial{x^{(1)}}}\right) =\textrm{\bf dim}(x^{(1)})$,
%\begin{align}
%\textrm{\bf dim}(\bf{\hat{y}} - \bf{y})                =& 1 \times D_{y} \\
%\textrm{\bf dim}(\bf \textrm{W}^{\top}_{2})      =& D_{y} \times H\\
%\textrm{\bf dim}(\bf{x^{(2)}\cdot(1-x^{(2)})})   =& 1 \times H \\
%\textrm{\bf dim}({\bf \textrm{W}^{\top}_{1}})   =& H \times D_{x}\\
%\end{align}
%So,
%\begin{align}
%\textrm{\bf dim}\left((\bf{\hat{y}} - \bf{y})\bf \textrm{W}^{\top}_{2}\right)                            =& (1 \times H) \\
%\textrm{\bf dim}\left((\bf{x^{(2)}\cdot(1-x^{(2)})}{\bf \textrm{W}^{\top}_{1}}\right)             =& (H \times D_{x})\\
%\end{align}
%It follows that, 
%\begin{equation}
%\textrm{\bf dim}\left(\frac{\partial{J}}{\partial{x^{(1)}}}\right) =  (1 \times H) (H \times D_{x}) = (1\times D_{x})
%\end{equation}
%As required.

\clearpage

\myhwtitle{1}{2(d)}{Gregory King}
\bigskip
\noindent How many parameters are there in this neural network, assuming the input is $\textrm{D}_{x}$-dimensional,
the output is $\textrm{D}_{y}$-dimensional, and there are H hidden units?\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\noindent $\bf{\textrm{W}}_{1}$ must have dimensions:  $\textrm{D}_{x}\times\textrm{H}$. The bias ($\bf{b}_{1}$) for the first layer must have
dimensions $\textrm{H}$. Adding these two together, yields $(\textrm{D}_{x} + 1)\times\textrm{H}$. Proceeding to the second layer,
there must be $\textrm{H}\times\textrm{D}_{y}$ parameters associated with the weight matrix ${\bf\textrm{W}}_{2}$. The bias ($\bf{b}_{2}$)
for the second layer must have dimensions $\textrm{D}_{y}$ elements. This yields, 
\begin{equation}
(\textrm{D}_{x} + 1)\times\textrm{H} + \textrm{D}_{y}\times(\textrm{H}+1)
\end{equation}
weights, for each input vector of dimensions $\textrm{D}_{x}$.
%\begin{itemize}
%\item[$\bf{\textrm{W}}_{1}$:]{$\textrm{D}_{x}\times\textrm{H}$}
%\item[$\bf{b}_{1}$:]{$\textrm{H}$}
%\item[${\bf\textrm{W}}_{2}$:]{$\textrm{H}\times\textrm{D}_{y}$}
%\item[$\bf{b}_{2}$:]{$\textrm{D}_{y}$}
%\end{itemize}
\clearpage
\myhwtitle{1}{2(e)}{Gregory King}
\bigskip
\noindent Fill in the implementation for the sigmoid activation function and its gradient in  \texttt{q2\_sigmoid.py}.
Test your implementation using python \texttt{q2\_sigmoid.py}. \textbf{Note:} Again, thoroughly test your code as the provided
tests may not be exhaustive.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\clearpage
\myhwtitle{1}{2(f)}{Gregory King}
\bigskip
\noindent To make debugging easier, we will now implement a gradient checker. Fill in the implementation
for \texttt{gradcheck\_naive} in \texttt{q2\_gradcheck.py}. Test your code using python \texttt{q2\_gradcheck.py}\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\clearpage
\myhwtitle{1}{2(g)}{Gregory King}
\bigskip
\noindent  Now, implement the forward and backward passes for a neural network with one sigmoid
hidden layer. Fill in your implementation in \texttt{q2\_neural.py}. Sanity check your implementation with
\texttt{q2\_neural.py}. \\
\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\clearpage


\myrunninghwhead{1}{3 (word2vec)}

\myhwtitle{1}{3(a)}{Gregory King}
\bigskip
\noindent Assume you are given a predicted word vector ${\boldsymbol v}_{c}$ corresponding to the center word $c$ for
\texttt{skipgram}, and word prediction is made with the \texttt{softmax} function found in \texttt{word2vec} models
\begin{equation}
{\hat{\boldsymbol y}}_{\boldsymbol o} = p(~{\boldsymbol o} \mid {\boldsymbol c}) = \frac{\exp{({\boldsymbol u}^{\top}_{o}{\boldsymbol v}_{c})}}{\sum^{\vert{W}\vert}_{j=1}\exp{({\boldsymbol u}^{\top}_{j}{\boldsymbol v}_{c})}}
\end{equation}
where $w$ denotes the $w$-th word and ${\boldsymbol u}_{w}$ ($w=1,...,\vert\textrm{W}\vert$)  are the `output' word vectors for all words in the vocabulary ($v^{\prime}_{w}$ in the lecture notes). Assume the cross entropy cost is applied to this prediction and word $o$ is the expected word (the $o$-th element of the one-hot label vector is one), derive the gradients with respect to ${\boldsymbol v}_{c}$.\\

\noindent \textbf{Hint:} It will be helpful to use notation from question 2. For instance, letting ${\hat{\boldsymbol y}}$ be the vector of the softmax
prediction for every word, ${\boldsymbol y}$ as the expected word vector, and the loss function
\begin{equation}
J_{\textrm{softmax-CE}}({\boldsymbol o}, {\boldsymbol v}_{c}, {\boldsymbol U}) = CE({\boldsymbol y},{\hat{\boldsymbol y}})
\end{equation}
where ${\boldsymbol U} = [ {\boldsymbol u}_{1}, {\boldsymbol u}_{2},...,{\boldsymbol u}_{\vert\textrm{W}\vert}]$ is the matrix of all the output vectors. \textit{Make sure you state the orientation of your vectors and matrices.}
\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
%TODO FIX this 
Applying cross-entropy cost to the above softmax probability defined above:
\begin{equation}
J =-\log{\boldsymbol P} = - {\boldsymbol w}_{i}^{\top}\hat{\boldsymbol r} + \log\sum^{\vert{V}\vert}_{j=1}\exp{({\boldsymbol w}^{\top}_{j}\hat{\boldsymbol r})}
\end{equation}
Let $z_{j}={\boldsymbol w}_{j}^{\top}\hat{\boldsymbol r}$, and $\delta^{i}_{j}$ be the indicator function (Kronecker delta), then
\begin{equation}
\frac{\partial J}{\partial{z_{k}}} = - \delta^{i}_{k} + \frac{\exp{({\boldsymbol w}^{\top}_{i}\hat{\boldsymbol r})}}{\sum^{\vert{V}\vert}_{j=1}\exp{({\boldsymbol w}^{\top}_{j}\hat{\boldsymbol r})}}
\end{equation}
Now, using the chain rule, we can calculate,
\begin{align}
\frac{\partial J}{\partial{\hat{\boldsymbol r}}} =&\frac{\partial J}{\partial{{\boldsymbol z}}}\frac{\partial{{\boldsymbol z}}}{\partial{\hat{\boldsymbol r}}} \\
                                                                    =& \sum^{\vert{V}\vert}_{j=1} {\boldsymbol w}_{j}\left(\frac{e^{z_{j}}}{\sum^{\vert{V}\vert}_{k=1}e^{z_{k}}} -  1\right) \\
                                                                    =&\sum^{\vert{V}\vert}_{k=1}{\boldsymbol P}( {\boldsymbol w}_{j} \vert \hat{\boldsymbol r} ) {\boldsymbol w}_{j} -  {\boldsymbol w}_{j}
\end{align}

\clearpage
\myhwtitle{1}{3(b)}{Gregory King}
\bigskip
\noindent In the previous problem, derive gradients for the `output' word vectors ${\boldsymbol w}_{j}$'s (including ${\boldsymbol w}_{i}$).\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
This follows immediately from the chain rule:
\begin{align}
\frac{\partial J}{\partial{\boldsymbol w}_{j}} = &\frac{\partial J}{\partial{{\boldsymbol z}}}\frac{\partial{{\boldsymbol z}}}{\partial{\boldsymbol w}_{j}} \\
                                                                  = & \hat{\boldsymbol r}\left(\frac{\exp{({\boldsymbol w}^{\top}_{i}\hat{\boldsymbol r})}}{\sum^{\vert{V}\vert}_{j=1}\exp{({\boldsymbol w}^{\top}_{j}\hat{\boldsymbol r})}} - \delta^{i}_{j}\right)
\end{align}
\clearpage

\myhwtitle{1}{3(c)}{Gregory King}
\bigskip
\noindent Repeat part (a) and (b) assuming we are using the negative sampling loss for the predicted vector $\hat{\boldsymbol r}$, and 
the expected output word is ${\boldsymbol w}_{i}$. Assume that $K$ negative samples are drawn, and they are ${\boldsymbol w}_{1},...,{\boldsymbol w}_{k}$, respectively for simplicity of notation ($k\in\{1,...,K\}$). Recall that the negative sampling loss function in this case is,
\begin{equation}
J(\hat{\boldsymbol r}, {\boldsymbol w}_{i}, {\boldsymbol w}_{1..K}) = -\log(\sigma( {\boldsymbol w}^{\top}_{i}\hat{\boldsymbol r})) - \sum^{K}_{k=1}\log(\sigma(- {\boldsymbol w}^{\top}_{i}\hat{\boldsymbol r}))
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function.\\

\noindent After you've done this, describe with one sentence why this cost function is much more efficient to
compute than the softmax-CE (you could provide a speed-up ratio, i.e. the runtime of the softmax-CE loss divided by the
runtime of the negative sampling loss).\\

\noindent {\bf Note}: the cost function here is the negative of what Mikolov \textit{et. al.} had in their original paper,
because we are doing a minimization instead of maximization in our code.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
Define $z_{i} \equiv {\boldsymbol w}^{\top}_{i}\hat{\boldsymbol r}$ and let $\delta^{i}_{j}$ be the Kronecker delta. If $i \notin \{1,...,K\}$
\begin{align}
\frac{\partial J}{\partial{z_{i}}} =& ~-\frac{\partial}{\partial{z_{i}}}(\log(\sigma(z_{i})) = -\frac{\sigma^{\prime}(z_{i})}{\sigma(z_{i})} \\
                                               =& ~-\frac{\sigma(z_{i})(1 - \sigma(z_{i}))}{\sigma(z_{i})} \\
                                               =& ~\sigma(z_{i}) - 1
\end{align}
If $i \in \{1,...,K\}$ then we need to take the derivative of the second term. It follows,
\begin{align}
\frac{\partial J}{\partial{z_{i}}} =& -\frac{\partial}{\partial{z_{i}}}\log(\sigma(-z_{i})) = -\frac{\sigma^{\prime}(-z_{i})}{\sigma(-z_{i})}\\
                                              =&  \frac{\sigma(-z_{i})(1 - \sigma(-z_{i}))}{\sigma(-z_{i})} \\
                                              =& 1 - \sigma(-z_{i}) = \sigma(z_{i})
\end{align}
Which yields the following, where $j\in{1,...,K}$,
\begin{equation*}
\frac{\partial{J}}{\partial{z_{i}}} = \sigma(z_{i}) - \delta^{i}_{j}
\end{equation*}

\begin{align}
\frac{\partial J}{\partial{\boldsymbol w}_{i}} &= (\sigma(z_{i}) - \delta^{i}_{j})\hat{\boldsymbol r}\\
\frac{\partial J}{\partial\hat{\boldsymbol r}}  &= (\sigma(z_{i}) - \delta^{i}_{j}){\boldsymbol w}_{i}
\end{align}

\clearpage
\myhwtitle{1}{3(d)}{Gregory King}
\bigskip
\noindent Derive the gradients for all of the word vectors for \texttt{skip-gram} and \texttt{CBOW} (optional)
given a set of context words [$\texttt{word}_{i-\textbf{c}},...,\texttt{word}_{i-\textbf{1}},\texttt{word}_{i},\texttt{word}_{i+\textbf{1}},...,\texttt{word}_{i+\textbf{c}}$ ], where \textbf{c} is the context size. You can denote 
the `input' and `output' word vectors for $\texttt{word}_{k}$ as ${\boldsymbol v}_{w_{k}}$ and ${\boldsymbol v}^{\prime}_{w_{k}}$ respectively for
convenience. \\

{\bf Hint}: feel free to use $F({\boldsymbol v}_{w_{0}}\vert\hat{\boldsymbol r})$ as a placeholder for softmax-CE or negative sampling in this part - you'll
see that this is a useful abstraction for the coding part.\\

{\bf Recall}: that for a \texttt{skip-gram}, the cost for a context is:
\begin{equation}
J_{\texttt{skip-gram}}(\texttt{word}_{i-\textbf{c}},...,\texttt{word}_{i+\textbf{c}}) = \sum_{-c\leq j\leq c, j\neq 0}F({\boldsymbol v}^{\prime}_{w_{i+j}}\vert{\boldsymbol v}_{w_{i}})
\end{equation}
For a simpler variant of \texttt{CBOW}, we sum up the input word vectors in the context,
\begin{equation}
\hat{\boldsymbol r} = \sum_{-c\leq j\leq c, j\neq 0} {\boldsymbol v}_{w_{i+j}}\label{eq:3d r def}
\end{equation}
\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
The \texttt{skip-gram} model involves summing over context (and treating each word as having a conditional probability). This yields
\begin{equation*}
\frac{\partial{J}}{\partial{{\boldsymbol v}_{w_{i}}}} = \frac{\partial}{\partial{{\boldsymbol v}_{w_{i}}}} \sum_{-c\leq j\leq c, j\neq 0} F({\boldsymbol v}^{\prime}_{w_{k+j}}\vert{\boldsymbol v}_{w_{k}}) = 
\begin{cases}
 \sum\frac{\partial F({\boldsymbol v}^{\prime}_{w_{j}}\vert{\boldsymbol v}_{w_{i}})}{\partial{{\boldsymbol v}_{w_{i}}}}\\
 \frac{\partial F({\boldsymbol v}^{\prime}_{w_{k}}\vert{\boldsymbol v}_{w_{i}})}{\partial{{\boldsymbol v}_{w_{k}}}}
\end{cases}
\end{equation*}
Whereas in the \texttt{CBOW} model, the summing over context is handled internally to the probability model (in other words, the context vector is represented by the sum of the word vectors and the conditional probability is with respect to this sum). This means that cost, $J$, is represented as (with $\hat{\boldsymbol r}$ defined by Equation~\ref{eq:3d r def}),
\begin{equation}
J_{\texttt{CBOW}} =  F({\boldsymbol v}^{\prime}_{w_{i+j}}\vert\hat{\boldsymbol r})
\end{equation}

\begin{align}
\frac{\partial J_{\texttt{CBOW}}}{\partial{\boldsymbol v}_{w_{k}}} &= \\
\frac{\partial J_{\texttt{CBOW}}}{\partial\hat{\boldsymbol r}}         &=
\end{align}

\clearpage
\myrunninghwhead{1}{4 (Sentiment Analysis)}

\myhwtitle{1}{4(a)}{Gregory King}
\bigskip
\noindent Explain in less than three sentences why do we want to introduce regularization when doing classification (in fact, most machine learning tasks).\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
Regularization helps prevent over-fitting.
\clearpage

\myhwtitle{1}{4(b)}{Gregory King}
\bigskip
\noindent Plot the classification accuracy on the \texttt{dev} set with respoect to the regularization value, using a logarithmic scale on the $x$-axis. Briefly explain with less than three sentences what you see in the plot.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}


%%%% End solution and document %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

